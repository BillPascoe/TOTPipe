{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "473560d0-f791-4fc8-ab1f-62786959be60",
   "metadata": {},
   "source": [
    "This script uses OpenAI to identify paragraphs in a long text that aren't related to the topic, genre or style you are focussing on.\n",
    "It was developed for collections of OCR newspaper articles where some of the articles have reporting on a wide range of diverse topics (crime, weather, harvests, baking competitions, wars, floods, etc) but we are only wanting to collect reporting on a specific one of these and want to identify, assess and remove paragraphs on irrelevant matters. \n",
    "Use a sample of 15 to 30 paragraphs on the topic, or in the genre or style you are working with (less than 15 probably not enough to get a good sample. 50 or more probably adds very little benefit). The first part of this creates an 'embedding' signature. Then we loop through all the paragraphs we want to check in a large text and compare their embeddings to the 'signature'. The script outputs any paragraph (truncated if very long) that doesn't match the signature. You can adjust the threshold for how similar/different the paragraph is before it is flagged. Then you use the list of lines to find the paragraphs and decide whether to remove them from your text.\n",
    "\n",
    "Note:\n",
    "Once the embedding signature has been created it is saved in reference_embeddings.json. If you want to force it to recreate the signature, eg because you changed the example paragraphs, delete this file, or make a back up, or change its name.\n",
    "\n",
    "The 'unrelated' paragraphs should not simply be deleted without checking, as they are likely to contain things like subheadings or be about something which in isolation might seem unrelated, but are actually part of the whole narrative.\n",
    "\n",
    "Requires:\n",
    "!pip -q install openai numpy pandas\n",
    "\n",
    "Scoring:\n",
    "Each paragraph is assigned a score for how close a match it is to the signature score. Here's a general idea of how scores indicate closeness:\n",
    "\n",
    "1.00 → pointing the same way (near-identical meaning)\n",
    "0.00 → orthogonal (no meaningful semantic overlap)\n",
    "< 0.00 → opposed meanings (rare in practice for prose)\n",
    "\n",
    "0.70 – 0.85 : Extremely close\n",
    "Near paraphrase\n",
    "Same event, same action, different wording\n",
    "Reprints, summaries, or very tight thematic overlap\n",
    "\n",
    "0.60 – 0.70 : Clearly same genre\n",
    "Different incidents\n",
    "Same type of activity (pursuit, violence, reprisal)\n",
    "Different actors / places / dates\n",
    "\n",
    "0.50 – 0.60 : Borderline / contextual\n",
    "Indirect references\n",
    "Aftermath, official correspondence, commentary\n",
    "Mentions violence obliquely or as background\n",
    "\n",
    "0.40 – 0.50 : Weakly related\n",
    "Shares vocabulary or tone\n",
    "But not really about the same thing\n",
    "Might mention people or places without the genre action\n",
    "\n",
    "< 0.40 : Unrelated\n",
    "Different topic entirely\n",
    "Shipping, weather, stock prices, sport, politics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215595d3-803e-4fa1-b31f-926150814362",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, json, math\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict, Any, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e921d2fe-5c99-4fa3-96c1-4c51151ba98b",
   "metadata": {},
   "source": [
    "configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19eba594-0c3c-486d-abad-f3cda76db8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_MODEL = \"text-embedding-3-small\"  # or \"text-embedding-3-large\"\n",
    "BATCH_SIZE = 200  # 100–500 usually fine; depends on paragraph sizes\n",
    "\n",
    "# Similarity thresholds (tune these after one test run)\n",
    "T_LOW  = 0.40   # below this => UNRELATED\n",
    "T_HIGH = 0.55   # between low/high => REVIEW, above high => IN_GENRE\n",
    "\n",
    "# Optional: cache reference embeddings so you don't re-embed exemplars every run\n",
    "REFERENCE_CACHE_JSON = \"reference_embeddings.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0532b1b-36f8-43f6-b14d-bcd788dd0c5f",
   "metadata": {},
   "source": [
    "helpers (splitting + embedding + cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a2a2ab-ff5c-4b31-9c45-02f271023dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_paragraphs(text: str) -> List[str]:\n",
    "    \"\"\"Split on 1+ blank lines. Keeps paragraphs intact.\"\"\"\n",
    "    parts = re.split(r\"\\n\\s*\\n+\", text.strip())\n",
    "    return [p.strip() for p in parts if p.strip()]\n",
    "\n",
    "def truncate(text: str, max_chars: int = 500) -> str:\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    if len(text) <= max_chars:\n",
    "        return text\n",
    "    return text[:max_chars].rsplit(\" \", 1)[0] + \"…\"\n",
    "    \n",
    "def first_nonempty_line(text: str) -> str:\n",
    "    for line in text.splitlines():\n",
    "        s = line.strip()\n",
    "        if s:\n",
    "            return s\n",
    "    return \"\"\n",
    "\n",
    "def split_articles_by_hashline(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split file into articles separated by a line that is only hashes (e.g. #######).\n",
    "    Any line containing only # characters (and whitespace) counts as a separator.\n",
    "    \"\"\"\n",
    "    # Normalize newlines\n",
    "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    # Split on separator lines\n",
    "    parts = re.split(r\"(?m)^\\s*#+\\s*$\", text)\n",
    "    return [p.strip() for p in parts if p.strip()]\n",
    "\n",
    "def strip_citation_line(article_text: str) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Return (citation_line, body_without_citation).\n",
    "    Assumes first non-empty line is citation.\n",
    "    \"\"\"\n",
    "    lines = article_text.splitlines()\n",
    "    citation = \"\"\n",
    "    body_lines = []\n",
    "    found = False\n",
    "    for i, line in enumerate(lines):\n",
    "        if not found and line.strip():\n",
    "            citation = line.strip()\n",
    "            body_lines = lines[i+1:]\n",
    "            found = True\n",
    "            break\n",
    "    body = \"\\n\".join(body_lines).strip()\n",
    "    return citation, body\n",
    "\n",
    "def embed_texts(client: OpenAI, texts: List[str], model: str, batch_size: int = 200) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Embed a list of strings. Returns array shape (n, d) float32.\n",
    "    Skips empty strings (but you should avoid passing empties).\n",
    "    \"\"\"\n",
    "    vecs = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        # API supports array of strings in one request :contentReference[oaicite:1]{index=1}\n",
    "        resp = client.embeddings.create(model=model, input=batch)\n",
    "        vecs.extend([item.embedding for item in resp.data])\n",
    "    return np.array(vecs, dtype=np.float32)\n",
    "\n",
    "def cosine_sim_matrix(A: np.ndarray, B: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between each row in A and each row in B.\n",
    "    If embeddings are normalized (OpenAI embeddings are), cosine = dot product. :contentReference[oaicite:2]{index=2}\n",
    "    Returns (A_rows, B_rows).\n",
    "    \"\"\"\n",
    "    # For safety, normalize anyway (tiny cost, avoids surprises if you change models/providers)\n",
    "    A_norm = A / np.linalg.norm(A, axis=1, keepdims=True)\n",
    "    B_norm = B / np.linalg.norm(B, axis=1, keepdims=True)\n",
    "    return A_norm @ B_norm.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dde241a-39d5-426a-9a36-bbd60802ea23",
   "metadata": {},
   "source": [
    "build exemplar “signature” or load it if it already exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcddca3-0ef6-4827-b557-f8dc82d57f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_or_load_reference(exemplar_path: str) -> Dict[str, Any]:\n",
    "    if os.path.exists(REFERENCE_CACHE_JSON):\n",
    "        with open(REFERENCE_CACHE_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "            ref = json.load(f)\n",
    "        if ref.get(\"model\") == EMBED_MODEL:\n",
    "            print(f\"Loaded cached reference embeddings from {REFERENCE_CACHE_JSON} (model={EMBED_MODEL})\")\n",
    "            return ref\n",
    "        else:\n",
    "            print(\"Cache exists but model differs; rebuilding reference.\")\n",
    "\n",
    "    with open(exemplar_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw = f.read()\n",
    "\n",
    "    exemplars = split_paragraphs(raw)\n",
    "    if not exemplars:\n",
    "        raise ValueError(\"No exemplar paragraphs found. Ensure blank lines separate paragraphs.\")\n",
    "\n",
    "    client = OpenAI()\n",
    "    ex_vecs = embed_texts(client, exemplars, EMBED_MODEL, batch_size=BATCH_SIZE)\n",
    "\n",
    "    ref = {\n",
    "        \"model\": EMBED_MODEL,\n",
    "        \"exemplars\": [{\"id\": i, \"text\": t, \"embedding\": ex_vecs[i].tolist()} for i, t in enumerate(exemplars)],\n",
    "    }\n",
    "    with open(REFERENCE_CACHE_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(ref, f)\n",
    "    print(f\"Built and cached {len(exemplars)} exemplars → {REFERENCE_CACHE_JSON} (model={EMBED_MODEL})\")\n",
    "    return ref"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce1c87b-48df-4d5c-b998-f047bd1abb74",
   "metadata": {},
   "source": [
    "Run to create the 'signature' for this topic/genre/style "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba584dba-b5f0-4310-a29c-717216ed2a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this to your exemplar-paragraph file\n",
    "EXEMPLARS_TXT = \"SampleParagraphs.txt\"\n",
    "\n",
    "ref = build_or_load_reference(EXEMPLARS_TXT)\n",
    "exemplar_vecs = np.array([e[\"embedding\"] for e in ref[\"exemplars\"]], dtype=np.float32)\n",
    "\n",
    "print(\"Exemplars:\", len(ref[\"exemplars\"]), \" | Embedding dim:\", exemplar_vecs.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0154a975-ad16-4616-b3e1-42ce305cfdf4",
   "metadata": {},
   "source": [
    "scan the multi-article file and classify paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1ba7dc-aadc-4ef5-9052-0f4104e5bf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_articles_file(path: str) -> pd.DataFrame:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw = f.read()\n",
    "\n",
    "    articles = split_articles_by_hashline(raw)\n",
    "    print(\"Articles found:\", len(articles))\n",
    "\n",
    "    rows = []\n",
    "    all_paras = []\n",
    "    para_meta = []  # (article_idx, citation, para_idx_in_article, preview, full_para)\n",
    "\n",
    "    for a_i, art in enumerate(articles):\n",
    "        citation, body = strip_citation_line(art)\n",
    "        if not body:\n",
    "            continue\n",
    "        paras = split_paragraphs(body)\n",
    "        for p_i, p in enumerate(paras):\n",
    "            preview = truncate(p, 500)\n",
    "            all_paras.append(p)\n",
    "            para_meta.append((a_i, citation, p_i, preview, p))\n",
    "\n",
    "    if not all_paras:\n",
    "        return pd.DataFrame(columns=[\"article_id\",\"citation\",\"para_id\",\"score_max\",\"label\",\"preview\"])\n",
    "\n",
    "    client = OpenAI()\n",
    "    para_vecs = embed_texts(client, all_paras, EMBED_MODEL, batch_size=BATCH_SIZE)\n",
    "\n",
    "    sims = cosine_sim_matrix(para_vecs, exemplar_vecs)  # shape (num_paras, num_exemplars)\n",
    "    score_max = sims.max(axis=1)\n",
    "\n",
    "    for (a_i, citation, p_i, preview, p), s in zip(para_meta, score_max):\n",
    "        if s < T_LOW:\n",
    "            label = \"UNRELATED\"\n",
    "        elif s < T_HIGH:\n",
    "            label = \"REVIEW\"\n",
    "        else:\n",
    "            label = \"IN_GENRE\"\n",
    "\n",
    "        rows.append({\n",
    "            \"article_id\": a_i,\n",
    "            \"citation\": citation,\n",
    "            \"para_id\": p_i,\n",
    "            \"score_max\": float(s),\n",
    "            \"label\": label,\n",
    "            \"preview\": preview,\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows).sort_values([\"article_id\",\"para_id\"]).reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4d97f9-5bb2-4123-9d85-6839881d7bbc",
   "metadata": {},
   "source": [
    "Run comparison check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a453607-60ea-4f10-9d37-d3ffa4769546",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTICLES_TXT = \"CheckGomeroiTexts.txt\"  # your file with many articles separated by ####### lines\n",
    "\n",
    "df = scan_articles_file(ARTICLES_TXT)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e48b0c-70c6-4ac1-ad9c-8fafa57cbf8b",
   "metadata": {},
   "source": [
    "output only “unrelated” paragraphs (truncated at 500 char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081694b9-10df-40ec-bf56-9db4967376f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "unrelated = df[df[\"label\"] == \"UNRELATED\"][[\"article_id\",\"para_id\",\"score_max\",\"preview\",\"citation\"]]\n",
    "review = df[df[\"label\"] == \"REVIEW\"][[\"article_id\",\"para_id\",\"score_max\",\"preview\",\"citation\"]]\n",
    "unrelated = unrelated.sort_values(\"score_max\", ascending=True)\n",
    "review = review.sort_values(\"score_max\", ascending=True)\n",
    "\n",
    "print(\"UNRELATED:\", len(unrelated))\n",
    "print(\"REVIEW:\", len(review))\n",
    "\n",
    "unrelated.to_csv(\"unrelated_paragraphs.csv\", index=False)\n",
    "review.to_csv(\"review_paragraphs.csv\", index=False)\n",
    "\n",
    "print(\"UNRELATED - see CSV for full list.\")\n",
    "unrelated.head(30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a26babc-335c-46f6-a866-a78ca5d8af3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"REVIEW - see CSV for full list.\")\n",
    "review.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc097ca3-1d53-4c11-8987-77e3e0023a05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
