{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccdc45eb-5ccf-4cca-89f3-cafae26eff69",
   "metadata": {},
   "source": [
    "This reads in a file containing news articles, each seperated by a line of ############### hashes. These are OCR texts with many OCR errors in them. This script call AI to auto correct the bad OCR. The prompt is designed to be cautious to avoid over correction, hallucination, to preserve historic spellings. This doesn't mean it's foolproof. Since there are many errors in OCR anyway this will be much improved for further processing and reading, but better check important facts and quotes against the original. The main risk is that because the text looks good it may lull you into a false sense of security and mask errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6933dc62-377e-4cf2-8201-fca43f9cdc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import pathlib\n",
    "import re\n",
    "\n",
    "# ---------------------------\n",
    "# CONFIG\n",
    "# ---------------------------\n",
    "MAINFOLDER = \"./Texts/\"\n",
    "FILENAME = \"NewsArticles.txt\"\n",
    "\n",
    "INFILE = MAINFOLDER + FILENAME\n",
    "OUTFILE = MAINFOLDER + \"AICLEAN_\" + FILENAME\n",
    "\n",
    "MODEL = \"gpt-4.1\"\n",
    "\n",
    "# ---------------------------\n",
    "# MODE\n",
    "# ---------------------------\n",
    "# \"articles\" = current behavior (split on ###### delimiters)\n",
    "# \"paged_doc\" = treat entire file as a single document, keep page markers protected\n",
    "MODE = \"paged_doc\"  # <-- articles or paged_doc\n",
    "MODE = \"paged_doc\"\n",
    "\n",
    "# Matches lines like: ===== page-02.png =====  (exactly, per your OCR output)\n",
    "PAGE_MARKER_RE = re.compile(r\"(?m)^===== page-\\d+\\.png =====$\")\n",
    "\n",
    "# Chunking settings\n",
    "MAX_CHARS_PER_CHUNK = 7000\n",
    "LONG_ARTICLE_THRESHOLD = 12000  # only chunk if body exceeds this\n",
    "\n",
    "BASE_PROMPT = \"\"\"Here is a historical colonial 19th century text.\n",
    "It has OCR errors in it. Please correct the OCR errors.\n",
    "\n",
    "Do not change any words that are already valid.\n",
    "Do not change historical spellings, place names, names, titles, abbreviations, or numbers,\n",
    "unless they are part of an OCR error.\n",
    "Make the minimum number of edits.\n",
    "Prefer single-character fixes over word substitutions.\n",
    "\n",
    "Preserve the original formatting as much as possible (line breaks, indentation, headings,\n",
    "salutations, lists, signatures), except where a line break is clearly an OCR artefact\n",
    "(e.g., a word split across lines).\n",
    "\n",
    "If a word or short span is clearly not a valid English word or historical spelling\n",
    "(for example, random letter sequences, mixed-case junk, or digit–letter mixtures),\n",
    "treat it as an OCR error.\n",
    "\n",
    "If you can confidently infer the intended text from context, correct it.\n",
    "If the inference is uncertain, put the inferred text in square brackets.\n",
    "If it cannot be inferred, output [illegible].\n",
    "\n",
    "When uncertainty affects a verb phrase or grammatical construction,\n",
    "bracket the entire inferred phrase rather than a single word.\n",
    "\n",
    "Join words split by hyphen at line breaks when clearly a line-wrap hyphenation.\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_ARTICLES = \"\"\"\\\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_PAGED_DOC = \"\"\"\\\n",
    "IMPORTANT: Page marker lines like \"===== page-02.png =====\" are structural markers.\n",
    "Do not change these lines in any way (not spacing, not punctuation, not casing, not digits).\n",
    "Treat page markers as scan boundaries only; ignore them for meaning.\n",
    "Sentences and paragraphs may continue across a page marker.\n",
    "\"\"\"\n",
    "\n",
    "# ---------------------------\n",
    "# STYLE ADD-ONS\n",
    "# ---------------------------\n",
    "STYLE_NEWS = \"\"\"The text is a newspaper item. Preserve the original newspaper style, including headings,\n",
    "small-caps, and punctuation conventions, and do not modernise language or spelling.\"\"\"\n",
    "\n",
    "STYLE_GOV = \"\"\"The text is government correspondence/policy. Preserve formatting and structure such as\n",
    "headings, salutations, numbered clauses, marginal notes, file/reference numbers, and signatures.\n",
    "Do not rewrite prose or reflow lists into paragraphs. Preserve abbreviations and codes (e.g., \"No. 14/1841\",\n",
    "\"Encl.\", \"Ref.\", \"£ s. d.\") unless the OCR error is obvious.\"\"\"\n",
    "\n",
    "parts = [BASE_PROMPT.rstrip()]\n",
    "if MODE == \"paged_doc\":\n",
    "    parts.append(STYLE_GOV.rstrip())\n",
    "    parts.append(PROMPT_PAGED_DOC.rstrip())\n",
    "elif MODE == \"articles\":\n",
    "    parts.append(STYLE_NEWS.rstrip())\n",
    "    parts.append(PROMPT_ARTICLES.rstrip())\n",
    "else:\n",
    "    raise ValueError(f\"Unknown MODE: {MODE}\")\n",
    "\n",
    "parts.append(\"Return only the corrected text.\")\n",
    "PROMPT = \"\\n\\n\".join(p for p in parts if p)\n",
    "\n",
    "# Split articles on delimiter lines that are hashes (6+), e.g. \"########\"\n",
    "DELIM_LINE_RE = re.compile(r\"(?m)^[ \\t]*#{6,}[ \\t]*$\")\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# ---------------------------\n",
    "# HELPERS\n",
    "# ---------------------------\n",
    "def split_keep_delims(text: str):\n",
    "    \"\"\"Split text into [chunk, delim, chunk, delim, ...], keeping delimiter lines.\"\"\"\n",
    "    parts = []\n",
    "    last = 0\n",
    "    for m in DELIM_LINE_RE.finditer(text):\n",
    "        parts.append(text[last:m.start()])\n",
    "        parts.append(m.group(0))  # delimiter line\n",
    "        last = m.end()\n",
    "    parts.append(text[last:])\n",
    "    return parts\n",
    "\n",
    "\n",
    "def extract_citation_and_body(article: str):\n",
    "    \"\"\"\n",
    "    Citation = first non-empty line of the article.\n",
    "    Body = everything after that line (may start immediately or with newlines).\n",
    "    \"\"\"\n",
    "    article = article.strip(\"\\n\")\n",
    "    if not article.strip():\n",
    "        return \"\", \"\"\n",
    "\n",
    "    m = re.search(r\"(?m)^(?!\\s*$)(.*)$\", article)\n",
    "    if not m:\n",
    "        return \"\", \"\"\n",
    "\n",
    "    citation = m.group(1)\n",
    "    body = article[m.end(1):]  # everything after the citation line text\n",
    "    return citation, body\n",
    "\n",
    "\n",
    "def normalise_newlines_for_ocr(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Conservative cleanup to help continuity without flattening paragraphs.\n",
    "    - Normalize Windows newlines\n",
    "    - Collapse 3+ newlines -> 2 newlines (keep paragraph breaks)\n",
    "    - Join line-wrap hyphenations across 1–3 newlines when continuation is lowercase\n",
    "    - Collapse double newlines that look mid-sentence -> single newline (wider heuristic)\n",
    "    \"\"\"\n",
    "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "\n",
    "    # keep paragraphs but remove giant gaps\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "\n",
    "    # fix sacri-\\n\\nfice -> sacrifice (and similar), but avoid 1841-\\n42\n",
    "    text = re.sub(r\"([A-Za-z])-\\n{1,3}([a-z])\", r\"\\1\\2\", text)\n",
    "\n",
    "    # collapse blank line mid-sentence (heuristic)\n",
    "    text = re.sub(\n",
    "        r\"([A-Za-z0-9,;:\\)\\]\\\"'’”])\\n\\n(?=[A-Za-z0-9\\\"'‘’“”\\(\\[])\",\n",
    "        r\"\\1\\n\",\n",
    "        text,\n",
    "    )\n",
    "\n",
    "    return text\n",
    "\n",
    "def chunk_by_paragraphs(text: str, max_chars: int):\n",
    "    \"\"\"\n",
    "    Split text into chunks up to max_chars, preserving paragraphs (\\n\\n) as boundaries.\n",
    "    If a single paragraph is longer than max_chars, fall back to splitting on single newlines,\n",
    "    and if still too long, hard-slice.\n",
    "    \"\"\"\n",
    "    text = text.strip(\"\\n\")\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    paras = text.split(\"\\n\\n\")\n",
    "    chunks = []\n",
    "    buf = \"\"\n",
    "\n",
    "    def flush():\n",
    "        nonlocal buf\n",
    "        if buf:\n",
    "            chunks.append(buf)\n",
    "            buf = \"\"\n",
    "\n",
    "    for p in paras:\n",
    "        p = p.strip(\"\\n\")\n",
    "        if not p:\n",
    "            continue\n",
    "\n",
    "        candidate = p if not buf else (buf + \"\\n\\n\" + p)\n",
    "\n",
    "        if len(candidate) <= max_chars:\n",
    "            buf = candidate\n",
    "            continue\n",
    "\n",
    "        # buffer would overflow\n",
    "        flush()\n",
    "\n",
    "        # if paragraph itself fits, start new buffer with it\n",
    "        if len(p) <= max_chars:\n",
    "            buf = p\n",
    "            continue\n",
    "\n",
    "        # paragraph too big: split by single newline\n",
    "        lines = p.split(\"\\n\")\n",
    "        linebuf = \"\"\n",
    "        for ln in lines:\n",
    "            ln = ln.rstrip(\"\\n\")\n",
    "            cand2 = ln if not linebuf else (linebuf + \"\\n\" + ln)\n",
    "            if len(cand2) <= max_chars:\n",
    "                linebuf = cand2\n",
    "            else:\n",
    "                if linebuf:\n",
    "                    chunks.append(linebuf)\n",
    "                    linebuf = ln\n",
    "                else:\n",
    "                    # single line too long: hard slice\n",
    "                    s = ln\n",
    "                    while len(s) > max_chars:\n",
    "                        chunks.append(s[:max_chars])\n",
    "                        s = s[max_chars:]\n",
    "                    linebuf = s\n",
    "        if linebuf:\n",
    "            chunks.append(linebuf)\n",
    "\n",
    "    flush()\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def ocr_correct(body_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Responses API call using structured input blocks (prevents prompt echo).\n",
    "    \"\"\"\n",
    "    resp = client.responses.create(\n",
    "        model=MODEL,\n",
    "        input=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [{\"type\": \"input_text\", \"text\": PROMPT}],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\"type\": \"input_text\", \"text\": body_text}],\n",
    "            },\n",
    "        ],\n",
    "        temperature=0,\n",
    "        max_output_tokens=8000,\n",
    "    )\n",
    "    return resp.output_text\n",
    "\n",
    "# Detect obvious OCR garble at the *end* of a chunk (boundary risk)\n",
    "GARBAGE_TAIL_RE = re.compile(r\"(?:[^\\w\\s]|[»«\\^_<>]){2,}\\s*$\")\n",
    "\n",
    "def merge_boundary_garble(chunks, max_chars, overflow=800):\n",
    "    \"\"\"\n",
    "    If a chunk ends with obvious garble, merge it with the next chunk BEFORE sending to OpenAI.\n",
    "    This avoids needing any prefix/suffix trimming (which can drop text).\n",
    "    overflow allows a small size exceedance to preserve context.\n",
    "    \"\"\"\n",
    "    merged = []\n",
    "    i = 0\n",
    "    while i < len(chunks):\n",
    "        ch = chunks[i]\n",
    "\n",
    "        if i < len(chunks) - 1 and GARBAGE_TAIL_RE.search(ch):\n",
    "            nxt = chunks[i + 1]\n",
    "            candidate = ch.rstrip(\"\\n\") + \"\\n\" + nxt.lstrip(\"\\n\")\n",
    "            if len(candidate) <= max_chars + overflow:\n",
    "                merged.append(candidate)\n",
    "                i += 2\n",
    "                continue\n",
    "\n",
    "        merged.append(ch)\n",
    "        i += 1\n",
    "\n",
    "    return merged\n",
    "\n",
    "\n",
    "def ocr_correct_with_chunking(body_text: str) -> str:\n",
    "    \"\"\"\n",
    "    If body is long, chunk it by paragraphs and correct each chunk.\n",
    "    Rejoin with blank lines between chunks.\n",
    "\n",
    "    Strategy:\n",
    "    - No prefix/suffix expansion and NO trimming (prevents dropped/chopped text).\n",
    "    - If a chunk ends with obvious garble, merge it with the next chunk before sending.\n",
    "    - If an API call fails for a chunk, keep the original chunk (never drop content).\n",
    "    \"\"\"\n",
    "    body_text = body_text.strip(\"\\n\")\n",
    "    if not body_text:\n",
    "        return \"\"\n",
    "\n",
    "    if len(body_text) <= LONG_ARTICLE_THRESHOLD:\n",
    "        try:\n",
    "            return ocr_correct(body_text).strip(\"\\n\")\n",
    "        except Exception:\n",
    "            return body_text\n",
    "\n",
    "    chunks = chunk_by_paragraphs(body_text, MAX_CHARS_PER_CHUNK)\n",
    "    chunks = merge_boundary_garble(chunks, MAX_CHARS_PER_CHUNK)\n",
    "\n",
    "    corrected_chunks = []\n",
    "    for ch in chunks:\n",
    "        ch = ch.strip(\"\\n\")\n",
    "        if not ch:\n",
    "            continue\n",
    "        try:\n",
    "            corrected_chunks.append(ocr_correct(ch).strip(\"\\n\"))\n",
    "        except Exception:\n",
    "            # Never drop content\n",
    "            corrected_chunks.append(ch)\n",
    "\n",
    "    return \"\\n\\n\".join(corrected_chunks).strip(\"\\n\")\n",
    "\n",
    "def isolate_page_markers(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Ensure each page marker line is surrounded by blank lines so it becomes its own paragraph.\n",
    "    This helps chunk_by_paragraphs keep markers intact and avoids marker lines being merged\n",
    "    into surrounding text by preprocessing.\n",
    "    \"\"\"\n",
    "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "\n",
    "    # Add a blank line before and after each marker line (without changing the marker itself)\n",
    "    # This is conservative: even if already blank-lined, it remains stable.\n",
    "    text = re.sub(r\"(?m)^(===== page-\\d+\\.png =====)$\", r\"\\n\\n\\1\\n\\n\", text)\n",
    "\n",
    "    # Clean up any huge gaps that might be created\n",
    "    text = re.sub(r\"\\n{4,}\", \"\\n\\n\\n\", text)  # keep a little slack\n",
    "    return text\n",
    "\n",
    "\n",
    "def validate_page_markers(original_text: str, corrected_text: str):\n",
    "    \"\"\"\n",
    "    Basic integrity check: same number of markers, and markers still match the pattern.\n",
    "    If it fails, we print a warning with a small sample.\n",
    "    \"\"\"\n",
    "    orig = PAGE_MARKER_RE.findall(original_text)\n",
    "    corr = PAGE_MARKER_RE.findall(corrected_text)\n",
    "\n",
    "    if len(orig) != len(corr):\n",
    "        print(f\"WARNING: Page marker count changed: original={len(orig)} corrected={len(corr)}\")\n",
    "        # show a few examples of what the corrected markers look like (lines containing 'page-')\n",
    "        corr_lines = [ln for ln in corrected_text.splitlines() if \"page-\" in ln.lower() or \"===== \" in ln]\n",
    "        print(\"Sample corrected lines containing 'page-' or '=====':\")\n",
    "        for ln in corr_lines[:10]:\n",
    "            print(\"   \", ln)\n",
    "        return\n",
    "\n",
    "    # Optional: stricter check that the marker *sequence* looks identical\n",
    "    # (this assumes your OCR produced them consistently)\n",
    "    if orig != corr:\n",
    "        print(\"WARNING: Page markers are present but differ from the original sequence.\")\n",
    "\n",
    "# ---------------------------\n",
    "# MAIN\n",
    "# ---------------------------\n",
    "raw = pathlib.Path(INFILE).read_text(encoding=\"utf-8\")\n",
    "\n",
    "if MODE == \"paged_doc\":\n",
    "    # 1) Make markers safe paragraph units\n",
    "    raw2 = isolate_page_markers(raw)\n",
    "\n",
    "    # 2) Normalise line breaks etc (doesn't change marker lines themselves)\n",
    "    cleaned = normalise_newlines_for_ocr(raw2)\n",
    "\n",
    "    # 3) Correct whole doc with paragraph chunking\n",
    "    corrected = ocr_correct_with_chunking(cleaned)\n",
    "\n",
    "    # 4) Validate markers survived\n",
    "    validate_page_markers(raw2, corrected)\n",
    "\n",
    "    pathlib.Path(OUTFILE).write_text(corrected.strip() + \"\\n\", encoding=\"utf-8\")\n",
    "    print(f\"Done (paged_doc) -> {OUTFILE}\")\n",
    "\n",
    "else:\n",
    "    # Existing \"articles\" behavior\n",
    "    parts = split_keep_delims(raw)\n",
    "\n",
    "    out_parts = []\n",
    "\n",
    "    for part in parts:\n",
    "        # Keep delimiter lines unchanged\n",
    "        if DELIM_LINE_RE.fullmatch(part.strip(\"\\n\")):\n",
    "            continue  # delimiter is handled on join\n",
    "\n",
    "        if not part.strip():\n",
    "            continue\n",
    "\n",
    "        citation, body = extract_citation_and_body(part)\n",
    "\n",
    "        # If no citation found (rare), just correct whole thing (with chunking)\n",
    "        if not citation:\n",
    "            cleaned = normalise_newlines_for_ocr(part)\n",
    "            corrected = ocr_correct_with_chunking(cleaned)\n",
    "            out_parts.append(corrected.strip(\"\\n\"))\n",
    "            continue\n",
    "\n",
    "        # If no body, keep citation only\n",
    "        if not body.strip():\n",
    "            out_parts.append(citation.strip())\n",
    "            continue\n",
    "\n",
    "        # Preprocess body only, then OCR-correct (chunked if long)\n",
    "        body_for_model = normalise_newlines_for_ocr(body)\n",
    "        corrected_body = ocr_correct_with_chunking(body_for_model)\n",
    "\n",
    "        # Force exactly one blank line between citation and body\n",
    "        rebuilt = citation.rstrip() + \"\\n\\n\" + corrected_body.lstrip(\"\\n\")\n",
    "        out_parts.append(rebuilt.strip(\"\\n\"))\n",
    "\n",
    "    # Write output with standard separators\n",
    "    out_text = \"\\n\\n########\\n\\n\".join(out_parts).strip() + \"\\n\"\n",
    "    pathlib.Path(OUTFILE).write_text(out_text, encoding=\"utf-8\")\n",
    "\n",
    "    print(f\"Done -> {OUTFILE}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f73d0d6-51d9-455e-8e4d-a88ba7dd33c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
