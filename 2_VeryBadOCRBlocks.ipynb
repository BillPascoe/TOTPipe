{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5854b4df-1922-4d48-86a8-9eec2876076f",
   "metadata": {},
   "source": [
    "Identify larger blocks of text with very bad OCR. These are the sort of areas that might be in the corner of very damaged pages, or the original scan has very black sections, low contrast, so the text could not be distinguished well. AI does an excellent job of correcting a word or two, or a sentence with glitchy parts but most character ok. But these are sections AI cannot be expected to correct but these sections would still be processed and returned by AI correction as if they were sensible English, so would be hard to recognise as 'hallucinations'. This checker looks for blocks of very bad OCR but ignores small OCR errors, so that you can double check these sections and manually correct. The idea is AI handles the bulk of the easy stuff, but there's still a bit of very bad stuff that must be human corrected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c84940-e11c-41ff-86be-279481740e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "HASH_SPLIT_RE = re.compile(r'^\\s*#{4,}\\s*$', re.MULTILINE)  # line of >=4 hashes\n",
    "MULTISPACE_RE = re.compile(r'\\s+')\n",
    "WORD_RE = re.compile(r\"[A-Za-z]+(?:'[A-Za-z]+)?\")\n",
    "TOKEN_RE = re.compile(r\"\\S+\")\n",
    "\n",
    "# Common OCR junk characters / patterns\n",
    "JUNK_CHAR_RE = re.compile(r\"[«»•·¦¬©®°±×÷§¶€™“”‘’–…]|[\\uFFFD]\")\n",
    "REPEATED_PUNCT_RE = re.compile(r\"([^\\w\\s])\\1{2,}\")   # e.g. \"!!!\", \"???\", \"///\"\n",
    "WEIRD_MIX_RE = re.compile(r\"(?=.*[A-Za-z])(?=.*\\d)[A-Za-z0-9]+\")  # token with both letters+digits\n",
    "\n",
    "# A tiny list of very common English words to help detect \"real language\"\n",
    "COMMON_WORDS = {\n",
    "    \"the\",\"and\",\"to\",\"of\",\"in\",\"a\",\"that\",\"is\",\"was\",\"for\",\"on\",\"with\",\"as\",\"by\",\"at\",\n",
    "    \"it\",\"from\",\"this\",\"be\",\"are\",\"or\",\"an\",\"have\",\"not\",\"which\",\"but\",\"they\",\"has\",\n",
    "    \"were\",\"had\",\"their\",\"his\",\"her\",\"we\",\"you\",\"he\",\"she\",\"them\",\"been\",\"will\",\"would\",\n",
    "    \"there\",\"one\",\"all\",\"so\",\"if\",\"no\",\"into\",\"up\",\"out\",\"do\",\"did\",\"than\",\"then\"\n",
    "}\n",
    "\n",
    "@dataclass\n",
    "class BlockScore:\n",
    "    length_chars: int\n",
    "    alpha_ratio: float\n",
    "    non_ascii_ratio: float\n",
    "    punct_ratio: float\n",
    "    digit_ratio: float\n",
    "    vowel_ratio: float\n",
    "    avg_token_len: float\n",
    "    weird_mix_ratio: float\n",
    "    junk_char_hits: int\n",
    "    repeated_punct_hits: int\n",
    "    common_word_ratio: float\n",
    "    short_gibberish_token_ratio: float\n",
    "    score: float\n",
    "\n",
    "def split_articles(text: str) -> List[str]:\n",
    "    # Split on hash lines; keep only non-empty chunks\n",
    "    parts = [p.strip(\"\\n\") for p in HASH_SPLIT_RE.split(text)]\n",
    "    return [p for p in parts if p.strip()]\n",
    "\n",
    "def extract_citation_and_body(article: str) -> Tuple[str, str]:\n",
    "    lines = article.splitlines()\n",
    "    # citation = first nonblank line\n",
    "    citation = \"\"\n",
    "    start_idx = 0\n",
    "    for i, ln in enumerate(lines):\n",
    "        if ln.strip():\n",
    "            citation = ln.strip()\n",
    "            start_idx = i + 1\n",
    "            break\n",
    "    body = \"\\n\".join(lines[start_idx:]).strip(\"\\n\")\n",
    "    return citation, body\n",
    "\n",
    "def split_paragraphs(body: str) -> List[str]:\n",
    "    # Paragraphs separated by blank lines (one or more)\n",
    "    paras = [p.strip() for p in re.split(r\"\\n\\s*\\n+\", body) if p.strip()]\n",
    "    return paras\n",
    "\n",
    "def safe_div(a: float, b: float) -> float:\n",
    "    return a / b if b else 0.0\n",
    "\n",
    "def score_block(block: str) -> BlockScore:\n",
    "    s = block.strip()\n",
    "    length_chars = len(s)\n",
    "\n",
    "    # Tokens and words\n",
    "    tokens = TOKEN_RE.findall(s)\n",
    "    words = WORD_RE.findall(s)\n",
    "    lower_words = [w.lower() for w in words]\n",
    "\n",
    "    # Character classes\n",
    "    alpha = sum(ch.isalpha() for ch in s)\n",
    "    digits = sum(ch.isdigit() for ch in s)\n",
    "    punct = sum((not ch.isalnum()) and (not ch.isspace()) for ch in s)\n",
    "\n",
    "    non_ascii = sum(ord(ch) > 127 for ch in s)\n",
    "    vowels = sum(ch.lower() in \"aeiou\" for ch in s if ch.isalpha())\n",
    "\n",
    "    alpha_ratio = safe_div(alpha, length_chars)\n",
    "    digit_ratio = safe_div(digits, length_chars)\n",
    "    punct_ratio = safe_div(punct, length_chars)\n",
    "    non_ascii_ratio = safe_div(non_ascii, length_chars)\n",
    "    vowel_ratio = safe_div(vowels, alpha)\n",
    "\n",
    "    avg_token_len = safe_div(sum(len(t) for t in tokens), len(tokens))\n",
    "\n",
    "    weird_mix = sum(1 for t in tokens if WEIRD_MIX_RE.fullmatch(t))\n",
    "    weird_mix_ratio = safe_div(weird_mix, len(tokens))\n",
    "\n",
    "    junk_char_hits = len(JUNK_CHAR_RE.findall(s))\n",
    "    repeated_punct_hits = len(REPEATED_PUNCT_RE.findall(s))\n",
    "\n",
    "    common_hits = sum(1 for w in lower_words if w in COMMON_WORDS)\n",
    "    common_word_ratio = safe_div(common_hits, len(lower_words))\n",
    "\n",
    "    # “short gibberish” tokens: mostly non-letters OR too few vowels for length\n",
    "    gib = 0\n",
    "    for t in tokens:\n",
    "        t_clean = re.sub(r\"\\W+\", \"\", t)\n",
    "        if not t_clean:\n",
    "            gib += 1\n",
    "            continue\n",
    "        letters = sum(ch.isalpha() for ch in t_clean)\n",
    "        v = sum(ch.lower() in \"aeiou\" for ch in t_clean if ch.isalpha())\n",
    "        if letters >= 4:\n",
    "            # very low vowel density often indicates garbage like \"strtiB\" or \"jfiff\"\n",
    "            if safe_div(v, letters) < 0.15:\n",
    "                gib += 1\n",
    "        else:\n",
    "            # short tokens: treat as gib if not mostly letters\n",
    "            if safe_div(letters, len(t_clean)) < 0.6:\n",
    "                gib += 1\n",
    "\n",
    "    short_gibberish_token_ratio = safe_div(gib, len(tokens))\n",
    "\n",
    "    # Combine into a single \"badness\" score\n",
    "    # We weight features that strongly indicate *very* bad OCR\n",
    "    score = 0.0\n",
    "    score += 2.5 * max(0.0, 0.70 - alpha_ratio)                 # too few letters overall\n",
    "    score += 2.0 * max(0.0, punct_ratio - 0.20)                 # punctuation overload\n",
    "    score += 1.8 * max(0.0, non_ascii_ratio - 0.02)             # weird chars / replacement glyphs\n",
    "    score += 1.2 * max(0.0, digit_ratio - 0.10)                 # digit soup\n",
    "    score += 2.0 * max(0.0, 0.25 - vowel_ratio)                 # consonant soup\n",
    "    score += 2.2 * max(0.0, short_gibberish_token_ratio - 0.25) # many garbage tokens\n",
    "    score += 1.3 * weird_mix_ratio                               # letter+digit tokens\n",
    "    score += 0.6 * min(5, junk_char_hits)                        # presence of OCR junk chars\n",
    "    score += 0.6 * min(5, repeated_punct_hits)                   # \"!!!\" \"///\" etc\n",
    "    score += 1.8 * max(0.0, 0.07 - common_word_ratio)            # basically no common words\n",
    "\n",
    "    return BlockScore(\n",
    "        length_chars=length_chars,\n",
    "        alpha_ratio=alpha_ratio,\n",
    "        non_ascii_ratio=non_ascii_ratio,\n",
    "        punct_ratio=punct_ratio,\n",
    "        digit_ratio=digit_ratio,\n",
    "        vowel_ratio=vowel_ratio,\n",
    "        avg_token_len=avg_token_len,\n",
    "        weird_mix_ratio=weird_mix_ratio,\n",
    "        junk_char_hits=junk_char_hits,\n",
    "        repeated_punct_hits=repeated_punct_hits,\n",
    "        common_word_ratio=common_word_ratio,\n",
    "        short_gibberish_token_ratio=short_gibberish_token_ratio,\n",
    "        score=score\n",
    "    )\n",
    "\n",
    "def is_very_bad(block: str, sc: BlockScore,\n",
    "                min_chars: int = 180,\n",
    "                min_score: float = 2.4) -> bool:\n",
    "    \"\"\"\n",
    "    Designed to catch *obviously* bad OCR blocks, not mild glitches.\n",
    "    \"\"\"\n",
    "    if sc.length_chars < min_chars:\n",
    "        return False\n",
    "\n",
    "    # Hard \"this is trash\" triggers (corner-damage patterns)\n",
    "    hard_triggers = (\n",
    "        sc.alpha_ratio < 0.55 and sc.punct_ratio > 0.18,\n",
    "        sc.non_ascii_ratio > 0.05,\n",
    "        sc.short_gibberish_token_ratio > 0.45,\n",
    "        sc.repeated_punct_hits >= 2,\n",
    "        sc.junk_char_hits >= 3\n",
    "    )\n",
    "\n",
    "    # If any two hard triggers are true, flag regardless of score.\n",
    "    if sum(bool(x) for x in hard_triggers) >= 2:\n",
    "        return True\n",
    "\n",
    "    # Otherwise, use the combined score threshold.\n",
    "    return sc.score >= min_score\n",
    "\n",
    "def find_bad_blocks_in_file(path: str,\n",
    "                            min_chars: int = 180,\n",
    "                            min_score: float = 2.4,\n",
    "                            max_blocks_per_article: int = 3) -> List[Dict]:\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    results = []\n",
    "    for art_idx, art in enumerate(split_articles(text), start=1):\n",
    "        citation, body = extract_citation_and_body(art)\n",
    "        if not body.strip():\n",
    "            continue\n",
    "\n",
    "        paras = split_paragraphs(body)\n",
    "        flagged = []\n",
    "\n",
    "        for p_idx, para in enumerate(paras, start=1):\n",
    "            sc = score_block(para)\n",
    "            if is_very_bad(para, sc, min_chars=min_chars, min_score=min_score):\n",
    "                flagged.append((p_idx, para, sc))\n",
    "\n",
    "        # Keep only top N worst per article (prevents noisy reports)\n",
    "        flagged.sort(key=lambda x: x[2].score, reverse=True)\n",
    "        flagged = flagged[:max_blocks_per_article]\n",
    "\n",
    "        for p_idx, para, sc in flagged:\n",
    "            results.append({\n",
    "                \"article_index\": art_idx,\n",
    "                \"citation\": citation,\n",
    "                \"paragraph_index\": p_idx,\n",
    "                \"bad_block\": para,\n",
    "                \"score\": sc.score,\n",
    "                \"alpha_ratio\": sc.alpha_ratio,\n",
    "                \"punct_ratio\": sc.punct_ratio,\n",
    "                \"non_ascii_ratio\": sc.non_ascii_ratio,\n",
    "                \"gib_ratio\": sc.short_gibberish_token_ratio,\n",
    "                \"common_word_ratio\": sc.common_word_ratio,\n",
    "            })\n",
    "\n",
    "    return results\n",
    "\n",
    "def print_report(results: List[Dict], show_scores: bool = True, max_chars: int = 900):\n",
    "    if not results:\n",
    "        print(\"No very-bad OCR blocks detected.\")\n",
    "        return\n",
    "\n",
    "    for i, r in enumerate(results, start=1):\n",
    "        print(\"=\"*90)\n",
    "        print(f\"{i}. {r['citation']}\")\n",
    "        print(f\"(Article #{r['article_index']}, Paragraph #{r['paragraph_index']})\")\n",
    "        if show_scores:\n",
    "            print(\n",
    "                f\"score={r['score']:.2f} | alpha={r['alpha_ratio']:.2f} | \"\n",
    "                f\"punct={r['punct_ratio']:.2f} | non_ascii={r['non_ascii_ratio']:.2f} | \"\n",
    "                f\"gib={r['gib_ratio']:.2f} | common={r['common_word_ratio']:.2f}\"\n",
    "            )\n",
    "        block = r[\"bad_block\"].strip()\n",
    "        if len(block) > max_chars:\n",
    "            block = block[:max_chars].rstrip() + \" …\"\n",
    "        print()\n",
    "        print(block)\n",
    "        print()\n",
    "\n",
    "# -----------------------------\n",
    "# Run\n",
    "# -----------------------------\n",
    "path = \"./Texts/NewsArticles.txt\"\n",
    "results = find_bad_blocks_in_file(path, min_chars=180, min_score=2.4, max_blocks_per_article=3)\n",
    "print_report(results, show_scores=True, max_chars=900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f335884b-5f3e-4cd0-aebc-12f3c5e8f78f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
